* multischedule
A multiple-asynchronous scheduling and delegation algorithm as part of CS223.

** Theoretical Structure 
Throughout the implementation, a "task" is defined by an atomic action with any number of parameters that returns a singlet value and performs most of its operations exclusively via side effects.

*** Load
Throughout all of designing a multiple-scheduling algorithm, we first need to define what actually to optimize for. This is largely arbitrary (referred by algorithms as "load"), but good metrics include "number of operations remaining", "number of tasks remaining", "CPU usage". All of the scheduling mechanisms wishes to achieve "load balance" --- roughly equal mean load across nodes.

*** Load Sharing Mechanics
To define an multiple-scheduling algorithm, we first need to define its mechanism to distribute tasks.

Note that, under all schemes, a "load balancer" could simply be a thread on each of the nodes running with some exclusive CPU carve-out---therefore any node within the cluster can load-balance itself, and any node outside can contact any node inside the cluster.

**** Sender-Initiated Algorithms
In **sender-initiated** systems, the processing cluster will immediately accept all sender delegated tasks; the load balancer will reschedule the task to the least-loaded node upon receipt of a task. Overcommited nodes will re-allocate unstarted tasks to the load-balancer to undercomitted nodes.

This has theoretically the least scheduling overhead, but the cluster may become overcommitted without any recourse as more tasks are added to the same node---as it requires compute resources to transfer a task.

**** Receiver-Initiated Algorithms
In **receiver-initiated** systems, the processing cluster will itself probe for tasks to complete from a shared queue outside the cluster. Instead of the sender, nodes---once becoming undercomitted---will request from the most overcommited node one of its unstarted tasks.

This method is more theoretically stable---overcommition is not a predicate for task transfers---but may cause many un-needed transfers between nodes.

**** Symmetric Search Algorithms
Load balancers, in this case, do both of the above. Under low load, a node does not actively poll for other, lower-loaded nodes to which to transfer the current task. Upon a certain load threshold, receiver-initated polling begins. All nodes are ready to both send and receive tasks.


** Implementation
In this system, we will provide an implementation of a multi-threaded, symmetric search load sharing mechanism. In this implementation, we define "load" as a user-provided metric per task, and the load limit of a node is completely user defined. Therefore, the current load of a node is simply the "load" of each task multiplied by the number of tasks of that type the current node has in queue. As per discussed above, tasks are atomic, and no data sharing mutex facilities are provided.

There is a few important implementation details that makes this project particularly interesting.

This is a symmetric search mechanism, but polling is done through one line established user-side (sender-initiated). Why? According to the literature, low load situations call for sender-initiation much more frequently, and the presumption is that if the nodes are /not runnig/, there should be no load.

Therefore, the user initiates the first call, calling any node to begin polling:

#+begin_src clojure
(node/process *global-state* 'node-one)
#+end_src

#+RESULTS:
: #object[clojure.core.async.impl.channels.ManyToManyChannel 0x369fb02d "clojure.core.async.impl.channels.ManyToManyChannel@369fb02d"]

As seen above, it will return a many-to-many channel containing promises for results take able by blocking the execution thread via the channel at the user's leisure.

To actually schedule the tasks, the above call triggers the scheduler in =node.clj=. The function of note here is not =(node/process)=, but instead =process-simple=, the "simple" processor for hosts marked as =simple=. Tasks that come in first get dropped to the overall backlog of tasks of not already in there:

(L142, node.clj)

#+begin_src clojure
;; (let [tasks
(if (and (-> task nil? not)
                       (not (in? (:backlog self) task)))
                (conj (:backlog self) task)
                (:backlog self))
;; ])
#+end_src

Then, the system calls =process-backlog=, a function that relaxes the 0/1 knapsack problem by greedily solving it: paring as much high-load tasks as possible that would fit under the load limit, and relegating the rest to a new backlog:

(L57, node.clj)

#+begin_src clojure
(reduce (fn [history curr]
          (let [new-load (+ (first history) (:load curr))]
            (if (< new-load (- (:max-load self) (:load self)))
              (cons new-load
                    (cons curr (rest history)))
              history)))
        '(0)
        backlog)
#+end_src

This function hijacks the Clojure reducer, mapping and reducing over the old backlog to add up loads until adding new elements hits the load limit. The backlog is already sorted.

The function returns the tasks that should be processed ("in the knapsack"), and filters the backlog against those tasks to get the new backlog

Finally, =do-run= is called, whereby the actual processing channels are created:

(L88, node.clj)

#+begin_src clojure
(let [channel (async/to-chan tasks)
      backlog (async/to-chan backlog)
      results (async/chan)])
#+end_src

The node then first processes its main tasks---creating *sender initiation*---(that in =channel=), each in a =go= thread (=go= creates a unsafe unblocking thread in =core.async= for Clojure.) =channel= the backlog of tasks, =backlog= is any tasks that are a part of the new backlog, and =results= are the processed results.

(L104, node.clj)

#+begin_src clojure
(go (let [task (<! channel)] ;;... ))
#+end_src

As you can see, one task is taken off the =channel= queue (=(<! channel)=), and processed.

After each set of processing is finished, and while the threaded have not merged yet, we filter for nodes that are more free than the current one and sends the highest-value yet possible task from the backlog channel to that node (also in a new thread)---creating *receiver initiation*:

(L114, node.clj)

#+begin_src clojure
(go (let [next-node (reverse (sort-by #(- (:max-load %)
                                          (:load %)) (:nodes new-state)))]
      (if-let [next-task (<! backlog)]
        (async/admix mix (process new-state (:name (first next-node)) next-task)))))
#+end_src

The results are then collated by adding the promise channel to the results channel, tagged with the worker's name, and returned after merging the threads (via =(async/admix)=, which mixes the two results channels).

** Running
The program is admittedly a very unstable system at the moment, in that it is =one-shot= (meaning it performs a single step of load balancing before needing reset), as well as =non-threadsafe=, which means that no memory leaking is not guaranteed. Furthermore, most of the value in this assignment is viewable in this document + the source code.

However, it would be run-able if needed.

Begin by installing the Clojure stack; use your favorite package manager to install:

#+begin_src bash
$(PKG_MGR) $(INSTALL_OP) clojure leiningen
#+end_src

on macOS, this would be:

#+begin_src bash
brew install clojure leiningen
#+end_src

Because Java is a little fussy, it would be ideal but not required that you don't have an existing copy of Java on your system. However, your package manager, as a part of installing leiningen, /should/ detect and configure whatever Java you have, and install the recommended one (=openJDK 18=) if missing.

Navigate to the folder of the project, and execute:

#+begin_src bash
lein run
#+end_src

If this is your first run of a Clojure project, it may take minutes (and gigabytes) for all the Java =.jar= used to support Clojure is pulled in.

To distinguish compile time and run time (which =lein= makes transparent), this program prints "Hello!" at the top of the mainloop to demarcate program start.
